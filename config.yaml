training:
  batch_size: 64
  epochs: 5
  lr: 0.001   # {SGD: 0.1, Adam: 0.001}
  optimizer: Adam   # SGD or Adam
  model_name: CIFAR10_CNN   # [DNN, MNIST_CNN, CIFAR10_CNN]
  dataset_name: CIFAR_10   # [MNIST, CIFAR_10]

FL:
  global_rounds: 10
  num_clients: 10
  dirichlet: 0.1   # [0.1, 1e20]
  ite_num: 1
  cohort: 5
